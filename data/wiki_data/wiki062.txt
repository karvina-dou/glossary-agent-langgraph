Incomputer networking,link aggregationis the combining (aggregating) of multiple network connections in parallel by any of several methods. Link aggregation increases totalthroughputbeyond what a single connection could sustain, and providesredundancywhere all but one of the physical links may fail without losing connectivity. Alink aggregation group(LAG) is the combined collection of physical ports.
Other umbrella terms used to describe the concept includetrunking,bundling,bonding,channelingorteaming.
Implementation may follow vendor-independent standards such asLink Aggregation Control Protocol(LACP) forEthernet, defined inIEEE 802.1AXor the previousIEEE 802.3ad, but alsoproprietary protocols.
Link aggregation increases the bandwidth and resilience ofEthernetconnections.
Bandwidth requirements do not scale linearly. Ethernet bandwidths historically have increased tenfold each generation:10Mbit/s,100 Mbit/s,1000 Mbit/s,10000Mbit/s. If one started to bump into bandwidth ceilings, then the only option was to move to the next generation, which could be cost prohibitive. An alternative solution, introduced by many of the network manufacturers in the early 1990s, is to use link aggregation to combine two physical Ethernet links into one logical link. Most of these early solutions required manual configuration and identical equipment on both sides of the connection.
There are threesingle points of failureinherent to a typical port-cable-port connection, in either a computer-to-switch or a switch-to-switch configuration: the cable itself or either of the ports the cable is plugged into can fail. Multiple logical connections can be made, but many of thehigher level protocolswere not designed tofail overcompletely seamlessly. Combining multiple physical connections into one logical connection using link aggregation provides more resilient communications.
Network architectscan implement aggregation at any of the lowest three layers of theOSI model. Examples of aggregation at layer 1 (physical layer) includepower line(e.g.IEEE 1901) andwireless(e.g. IEEE 802.11) network devices that combine multiple frequency bands. OSI layer 2 (data link layer, e.g.Ethernet framein LANs ormulti-link PPPin WANs, EthernetMAC address) aggregation typically occurs across switch ports, which can be either physical ports or virtual ones managed by an operating system. Aggregation at layer 3 (network layer) in the OSI model can useround-robin scheduling, hash values computed from fields in the packet header, or a combination of these two methods.
Regardless of the layer on which aggregation occurs, it is possible to balance the network load across all links. However, in order to avoidout-of-order delivery, not all implementations take advantage of this. Most methods providefailoveras well.
Combining can either occur such that multiple interfaces share one logical address (i.e. IP) or one physical address (i.e. MAC address), or it allows each interface to have its own address. The former requires that both ends of a link use the same aggregation method, but has performance advantages over the latter.
Channel bonding is differentiated fromload balancingin that load balancing divides traffic between network interfaces on pernetwork socket(layer 4) basis, while channel bonding implies a division of traffic between physical interfaces at a lower level, either per packet (layer 3) or a data link (layer 2) basis.
By the mid-1990s, most network switch manufacturers had included aggregation capability as a proprietary extension to increase bandwidth between their switches.  Each manufacturer developed its own method, which led to compatibility problems. TheIEEE 802.3working group took up a study group to create an interoperablelink layerstandard (i.e. encompassing the physical and data-link layers both) in a November 1997 meeting.The group quickly agreed to include an automatic configuration feature which would add in redundancy as well. This became known asLink Aggregation Control Protocol(LACP).
As of 2000, most gigabit channel-bonding schemes used the IEEE standard of link aggregation which was formerly clause 43 of theIEEE 802.3standard added in March 2000 by the IEEE 802.3ad task force.Nearly every network equipment manufacturer quickly adopted this joint standard over their proprietary standards.
The 802.3 maintenance task force report for the 9th revision project in November 2006 noted that certain 802.1 layers (such as802.1Xsecurity) were positioned in theprotocol stackbelow link aggregation which was defined as an802.3sublayer.To resolve this discrepancy, the 802.3ax (802.1AX) task force was formed,resulting in the  formal transfer of the protocol to the 802.1 group with the publication of IEEE 802.1AX-2008 on 3 November 2008.As of February 2025 the current revision of the standard is802.1AX-2020.For an overview of the history of the 802.1AX standard, seethis partof the table on theIEEE 802.1family of standards.
Within the IEEE Ethernet standards, the Link Aggregation Control Protocol (LACP) provides a method to control the bundling of several physical links together to form a single logical link. LACP allows a network device to negotiate an automatic bundling of links by sending LACP packets to their peer, a directly connected device that also implements LACP.
LACP Features and practical examples
LACP works by sending frames (LACPDUs) down all links that have the protocol enabled. If it finds a device on the other end of a link that also has LACP enabled, that device will independently send frames along the same links in the opposite direction enabling the two units to detect multiple links between themselves and then combine them into a single logical link. LACP can be configured in one of two modes: active or passive. In active mode, LACPDUs are sent 1 per second along the configured links. In passive mode, LACPDUs are not sent until one is received from the other side, a speak-when-spoken-to protocol.
In addition to the IEEE link aggregation substandards, there are a number of proprietary aggregation schemes including Cisco'sEtherChannelandPort Aggregation Protocol, Juniper's Aggregated Ethernet, AVAYA'sMulti-Link Trunking,Split Multi-Link Trunking,Routed Split Multi-Link TrunkingandDistributed Split Multi-Link Trunking, ZTE's Smartgroup, Huawei's Eth-Trunk, andConnectify's Speedify.Most high-end network devices support some form of link aggregation. Software-based implementations – such as the*BSDlaggpackage,Linuxbondingdriver,Solarisdladm aggr, etc. – exist for many operating systems.
The Linuxbondingdriverprovides a method for aggregating multiplenetwork interface controllers(NICs) into a single logical bonded interface of two or more so-called(NIC) slaves. The majority of modernLinux distributionscome with aLinux kernelwhich has the Linux bonding driver integrated as aloadable kernel moduleand theifenslave(if = [network] interface)user-levelcontrol program pre-installed.Donald Beckerprogrammed the original Linux bonding driver. It came into use with theBeowulf clusterpatches for theLinuxkernel 2.0.
Modes for the Linux bonding driver(network interface aggregation modes) are supplied as parameters to the kernel bonding module at load time. They may be given as command-line
arguments to theinsmodormodprobecommands, but are usually specified in a Linux distribution-specific configuration file. The behavior of the single logical bonded interface depends upon its specified bonding driver mode. The default parameter is balance-rr.
The Linux Team driverprovides an alternative to bonding driver. The main difference is that Team driver kernel part contains only essential code and the rest of the code (link validation, LACP implementation, decision making, etc.) is run inuserspaceas a part ofteamddaemon.
Link aggregation offers an inexpensive way to set up a high-capacitybackbone networkthat transfers multiple times more data than any single port or device can deliver. Link aggregation also allows the network's backbone speed to grow incrementally as demand on the network increases, without having to replace everything and deploy new hardware.
Most backbone installations install more cabling or fiber optic pairs than is initially necessary. This is done because labor costs are higher than the cost of the cable, and running extra cable reduces future labor costs if networking needs change. Link aggregation can allow the use of these extra cables to increase backbone speeds for little or no extra cost if ports are available.
When balancing traffic, network administrators often wish to avoid reordering Ethernet frames. For example, TCP suffers additional overhead when dealing with out-of-order packets. This goal is approximated by sending all frames associated with a particular session across the same link. Common implementations use L2 or L3 hashes (i.e. based on the MAC or the IP addresses), ensuring that the same flow is always sent via the same physical link.
However, this may not provide even distribution across the links in the trunk when only a single or very few pairs of hosts communicate with each other, i.e. when the hashes provide too little variation. It effectively limits the client bandwidth in aggregate.In the extreme, one link is fully loaded while the others are completely idle and aggregate bandwidth is limited to this single member's maximum bandwidth. For this reason, an even load balancing and full utilization of all trunked links is almost never reached in real-life implementations.
NICs trunked together can also provide network links beyond the throughput of any one single NIC. For example, this allows a central file server to establish an aggregate 2-gigabit connection using two 1-gigabit NICs teamed together. Note the data signaling rate will still be1 Gbit/s, which can be misleading depending on methodologies used to test throughput after link aggregation is employed.
Microsoft WindowsServer 2012 supports link aggregation natively. Previous Windows Server versions relied on manufacturer support of the feature within theirdevice driversoftware.Intel, for example, released Advanced Networking Services (ANS) to bond Intel Fast Ethernet and Gigabit cards.
Nvidiasupports teaming with their Nvidia Network Access Manager/Firewall Tool.HPhas a teaming tool for HP-branded NICs which supports several modes of link aggregation including 802.3ad with LACP. In addition, there is a basic layer-3 aggregationthat allows servers with multiple IP interfaces on the same network to perform load balancing, and for home users with more than one internet connection, to increase connection speed by sharing the load on all interfaces.
Broadcomoffers advanced functions via Broadcom Advanced Control Suite (BACS), via which the teaming functionality of BASP (Broadcom Advanced Server Program) is available, offering 802.3ad static LAGs, LACP, and "smart teaming" which doesn't require any configuration on the switches to work. It is possible to configure teaming with BACS with a mix of NICs from different vendors as long as at least one of them is from Broadcom and the other NICs have the required capabilities to support teaming.
Linux,FreeBSD,NetBSD,OpenBSD,macOS,OpenSolarisand commercial Unix distributions such asAIXimplement Ethernet bonding at a higher level and, as long as the NIC is supported by the kernel, can deal with NICs from different manufacturers or using different drivers.
Citrix XenServerandVMware ESXhave native support for link aggregation. XenServer offers both static LAGs as well as LACP. vSphere 5.1 (ESXi) supports both static LAGs and LACP natively with their virtual distributed switch.
Microsoft'sHyper-Vdoes not offer link aggregation support from the hypervisor level, but the above-mentioned methods for teaming under Windows apply to Hyper-V.
With the modesbalance-rr,balance-xor,broadcastand802.3ad, all physical ports in the link aggregation group must reside on the same logical switch, which, in most common scenarios, will leave a single point of failure when the physical switch to which all links are connected goes offline. The modesactive-backup,balance-tlb, andbalance-albcan also be set up with two or more switches. But after failover (like all other modes), in some cases, active sessions may fail (due to ARP problems) and have to be restarted.
However, almost all vendors have proprietary extensions that resolve some of this issue: they aggregate multiple physical switches into one logical switch. Nortel'ssplit multi-link trunking(SMLT) protocol allows multiple Ethernet links to be split across multiple switches in a stack, preventing any single point of failure and additionally allowing all switches to be load balanced across multiple aggregation switches from the single access stack. These devices synchronize state across anInter-Switch Trunk(IST) such that they appear to the connecting (access) device to be a single device (switch block) and prevent any packet duplication. SMLT provides enhanced resiliency with sub-second failover and sub-second recovery for all speed trunks while operating transparently to end-devices.
Multi-chassis link aggregation groupprovides similar features in a vendor-nonspecific manner. To the connected device, the connection appears as a normal link aggregated trunk. The coordination between the multiple sources involved is handled in a vendor-specific manner.
In most implementations, all the ports used in an aggregation consist of the same physical type, such as all copper ports (10/100/1000BASE‑T), all multi-mode fiber ports, or all single-mode fiber ports. However, all the IEEE standard requires is that each link be full duplex and all of them have an identical speed (10, 100, 1,000 or 10,000 Mbit/s).
Many switches are PHY independent, meaning that a switch could have a mixture of copper, SX, LX, LX10 or otherGBIC/SFPmodular transceivers. While maintaining the same PHY is the usual approach, it is possible to aggregate a 1000BASE-SX fiber for one link and a 1000BASE-LX (longer, diverse path) for the second link. One path may have a longer propagation time but since most implementations keep a single traffic flow on the same physical link (using a hash of either MAC addresses, IP addresses, or IP/transport-layer portcombinations as index) this doesn't cause problematicout-of-order delivery.
Aggregation mismatchrefers to not matching the aggregation type on both ends of the link. Some switches do not implement the 802.1AX standard but support static configuration of link aggregation. Therefore, link aggregation between similarly statically configured switches may work but will fail between a statically configured switch and a device that is configured for LACP.
OnEthernetinterfaces, channel bonding requires assistance from both the Ethernetswitchand the host computer'soperating system, which muststripethe deliveryof frames across the network interfaces in the same manner that I/O is striped across disks in aRAID 0array.For this reason, some discussions of channel bonding also refer toRedundant Array of Inexpensive Nodes(RAIN) or toredundant array of independent network interfaces.
In analog modems, multipledial-uplinks overPOTSmay be bonded. Throughput over such bonded connections can come closer to the aggregate bandwidth of the bonded links than can throughput under routing schemes which simply load-balance outgoing network connections over the links.
Similarly, multipleDSL linescan be bonded to give higher bandwidth; in theUnited Kingdom,ADSLis sometimes bonded to give for example512 kbit/supload bandwidth and4 Mbit/sdownload bandwidth, in areas that only have access to2 Mbit/sbandwidth.
Under the DOCSIS 3.0 and 3.1 specifications for data overcable TVsystems, multiple channels may be bonded. Under DOCSIS 3.0, up to 32 downstream and 8 upstream channels may be bonded.These are typically 6 or 8 MHz wide. DOCSIS 3.1 defines more complicated arrangements involving aggregation at the level of subcarriers and larger notional channels.
Broadbandbonding is a type of channel bonding that refers to aggregation of multiple channels atOSI layersat level four or above. Channels bonded can be wired links such as aT-1orDSL line. Additionally, it is possible to bond multiplecellular linksfor an aggregated wireless bonded link.
Other bonding methodologies reside at lower OSI layers, requiring coordination withtelecommunications companiesfor implementation. Broadband bonding, because it is implemented at higher layers, can be done without this coordination.
Commercial implementations of broadband channel bonding include:
On802.11(Wi-Fi), channel bonding is used inSuper Gtechnology, referred to as108 Mbit/s. It bonds two channels of standard802.11g, which has54 Mbit/sdata signaling rateper channel.
OnIEEE 802.11n, a mode with a channel width of 40 MHz is specified. This is not channel bonding, but a single channel with double the older 20 MHz channel width, thus using two adjacent 20 MHz bands. This allows direct doubling of the PHY data rate from a single 20 MHz channel.